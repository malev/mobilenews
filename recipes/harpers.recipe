__license__   = 'GPL v3'
__copyright__ = '2008-2010, Darko Miletic <darko.miletic at gmail.com>'
'''
harpers.org
'''
from calibre.web.feeds.news import BasicNewsRecipe

class Harpers(BasicNewsRecipe):
    title                 = u"Harper's Magazine"
    __author__            = u'Darko Miletic'
    language              = 'en'
    description           = u"Harper's Magazine: Founded June 1850."
    publisher             = "Harper's Magazine "
    category              = 'news, politics, USA'
    oldest_article        = 30
    max_articles_per_feed = 100
    no_stylesheets        = True
    use_embedded_content  = False

    conversion_options = {
                          'comment'   : description
                        , 'tags'      : category
                        , 'publisher' : publisher
                        , 'language'  : language
                        }

    extra_css = '''
                h1{ font-family:georgia ; color:#111111; font-size:large;}
                .box-of-helpful{ font-family:arial ; font-size:x-small;}
                p{font-family:georgia ;}
                .caption{font-family:Verdana,sans-serif;font-size:x-small;color:#666666;}
                '''

    keep_only_tags = [ dict(name='div', attrs={'id':'cached'}) ]
    remove_tags = [
                     dict(name='table', attrs={'class':['rcnt','rcnt topline']})
                    ,dict(name=['link','object','embed','meta','base'])
                  ]
    remove_attributes = ['width','height']

    feeds       = [(u"Harper's Magazine", u'http://www.harpers.org/rss/frontpage-rss20.xml')]

    def get_cover_url(self):
        cover_url = None
        index = 'http://harpers.org/'
        soup = self.index_to_soup(index)
        link_item = soup.find(name = 'img',attrs= {'class':"cover"})
        if link_item:
           cover_url = 'http://harpers.org' + link_item['src']
        return cover_url

    def preprocess_html(self, soup):
        for item in soup.findAll(style=True):
            del item['style']
        for item in soup.findAll(xmlns=True):
            del item['xmlns']
        return soup
